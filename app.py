import streamlit as st
import pandas as pd
import re
from io import StringIO, BytesIO
from datetime import datetime
import requests
from bs4 import BeautifulSoup

# âœ… í‚¤ì›Œë“œ ì„¤ì •
keywords = ["ì²œì¬êµìœ¡", "ì²œì¬êµê³¼ì„œ", "ì§€í•™ì‚¬", "ë²½í˜¸", "í”„ë¦°í”¼ì•„", "ë¯¸ë˜ì—”", "êµê³¼ì„œ", "ë™ì•„ì¶œíŒ"]

# âœ… ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í•¨ìˆ˜
def categorize(text):
    text = text.lower()
    if "ì´ë²¤íŠ¸" in text: return "ì´ë²¤íŠ¸"
    elif "í›„ì›" in text or "ê¸°íƒ" in text: return "í›„ì›"
    elif "ê¸°ë¶€" in text: return "ê¸°ë¶€"
    elif "í˜‘ì•½" in text or "mou" in text: return "í˜‘ì•½/MOU"
    elif any(w in text for w in ["ì—ë“€í…Œí¬", "ë””ì§€í„¸êµìœ¡", "aiêµìœ¡", "ìŠ¤ë§ˆíŠ¸êµìœ¡"]): return "ì—ë“€í…Œí¬/ë””ì§€í„¸êµìœ¡"
    elif "ì •ì±…" in text: return "ì •ì±…"
    elif "ì¶œíŒ" in text: return "ì¶œíŒ"
    elif "ì¸ì‡„" in text or "í”„ë¦°íŠ¸" in text: return "í”„ë¦°íŠ¸ ë° ì¸ì‡„"
    elif "ì±„ìš©" in text or "êµì‚¬" in text: return "ì¸ì‚¬/ì±„ìš©"
    elif "ê³µê¸‰" in text: return "ê³µê¸‰"
    elif "êµìœ¡" in text: return "êµìœ¡"
    else: return "ê¸°íƒ€"

# âœ… ë‰´ìŠ¤ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
def get_body_text(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, "lxml")
        return soup.get_text(" ", strip=True).lower()
    except:
        return ""

# âœ… ê¸°ì‚¬ ë‚ ì§œ ì¶”ì¶œ
def get_news_date(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, "lxml")
        for prop in ["article:published_time", "og:article:published_time"]:
            tag = soup.find("meta", {"property": prop})
            if tag and tag.get("content"):
                return tag["content"][:10].replace("-", ".")
        return "ë‚ ì§œ ì—†ìŒ"
    except:
        return "ë‚ ì§œ ì˜¤ë¥˜"

# âœ… ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜ (BeautifulSoupë§Œ ì‚¬ìš©)
def crawl_news(keyword):
    results = []
    seen_links = set()
    for page in range(1, 6):
        start = (page - 1) * 10 + 1
        url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sort=1&nso=so%3Add%2Cp%3A2w&start={start}"
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, "lxml")
        articles = soup.select(".news_area")
        if not articles:
            break
        for article in articles:
            try:
                title_elem = article.select_one(".news_tit")
                title = title_elem["title"]
                link = title_elem["href"]
                if link in seen_links:
                    continue
                seen_links.add(link)
                summary = article.select_one(".dsc_txt_wrap").text.strip() if article.select_one(".dsc_txt_wrap") else ""
                press = article.select_one(".info_group a").text.strip() if article.select_one(".info_group a") else ""
                body = get_body_text(link)
                full = summary + " " + body
                category = categorize(full)
                date = get_news_date(link)
                results.append({
                    "ì¶œíŒì‚¬ í‚¤ì›Œë“œ": keyword,
                    "ì¹´í…Œê³ ë¦¬": category,
                    "ë‚ ì§œ": date,
                    "ì œëª©": title,
                    "URL": link,
                    "ìš”ì•½": summary,
                    "ì–¸ë¡ ì‚¬": press
                })
            except:
                continue
    return results

# âœ… Streamlit ì•±
st.set_page_config(page_title="ë‰´ìŠ¤ í¬ë¡¤ë§ ë°°í¬ìš©", layout="wide")
st.title("ğŸ“° ì¶œíŒì‚¬ ê´€ë ¨ ë‰´ìŠ¤ í¬ë¡¤ë§ê¸° (ë°°í¬ìš©)")

kw_input = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„)", value=", ".join(keywords))
if st.button("í¬ë¡¤ë§ ì‹œì‘"):
    st.info("ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”... â³")
    final_results = []
    for kw in [k.strip() for k in kw_input.split(",") if k.strip()]:
        final_results.extend(crawl_news(kw))
    df = pd.DataFrame(final_results)
    st.success(f"ì´ {len(df)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ!")
    st.dataframe(df)
    excel = BytesIO()
    df.to_excel(excel, index=False, engine="openpyxl")
    st.download_button("â¬‡ï¸ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", excel.getvalue(), file_name="ì¶œíŒì‚¬_ë‰´ìŠ¤_í¬ë¡¤ë§_ê²°ê³¼.xlsx")
