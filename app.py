
import streamlit as st
import pandas as pd
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import re
import time
import chardet

# í‚¤ì›Œë“œ ì„¤ì •
keywords = ["ì²œì¬êµìœ¡", "ì²œì¬êµê³¼ì„œ", "ì§€í•™ì‚¬", "ë²½í˜¸", "í”„ë¦°í”¼ì•„", "ë¯¸ë˜ì—”", "êµê³¼ì„œ", "ë™ì•„ì¶œíŒ"]
category_keywords = {
    "í›„ì›": ["í›„ì›", "ê¸°íƒ"],
    "ê¸°ë¶€": ["ê¸°ë¶€"],
    "í˜‘ì•½/MOU": ["í˜‘ì•½", "mou"],
    "ì—ë“€í…Œí¬/ë””ì§€í„¸êµìœ¡": ["ì—ë“€í…Œí¬", "ë””ì§€í„¸êµìœ¡", "aiêµìœ¡", "ìŠ¤ë§ˆíŠ¸êµìœ¡"],
    "ì •ì±…": ["ì •ì±…"],
    "ì¶œíŒ": ["ì¶œíŒ"],
    "ì¸ì‚¬/ì±„ìš©": ["ì±„ìš©", "êµì‚¬"],
    "í”„ë¦°íŠ¸ ë° ì¸ì‡„": ["ì¸ì‡„", "í”„ë¦°íŠ¸"],
    "ê³µê¸‰": ["ê³µê¸‰"],
    "êµìœ¡": ["êµìœ¡"],
    "ì´ë²¤íŠ¸": ["ì´ë²¤íŠ¸", "ì‚¬ì€í’ˆ"]
}

def crawl_news_quick(keyword, pages=5):
    headers = {"User-Agent": "Mozilla/5.0"}
    results = []
    seen = set()
    today = datetime.today().date()
    two_weeks_ago = today - timedelta(days=14)

    for page in range(1, pages + 1):
        start = (page - 1) * 10 + 1
        url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sort=1&nso=so:dd,p:2w&start={start}"
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, "lxml")
        articles = soup.select(".news_area")
        for a in articles:
            try:
                title = a.select_one(".news_tit").get("title")
                link = a.select_one(".news_tit").get("href")
                summary = a.select_one(".dsc_txt_wrap").get_text(strip=True)
                press = a.select_one(".info_group a").get_text(strip=True)

                if link in seen or summary in seen:
                    continue
                seen.add(link)
                seen.add(summary)

                date = get_news_date(link)
                try:
                    if datetime.strptime(date, "%Y.%m.%d").date() < two_weeks_ago:
                        continue
                except:
                    continue

                full_text = (title + " " + summary).lower()

                results.append({
                    "ì¶œíŒì‚¬ëª…": check_publisher(full_text),
                    "ì¹´í…Œê³ ë¦¬": categorize_news(full_text),
                    "ë‚ ì§œ": date,
                    "ì œëª©": title,
                    "URL": link,
                    "ìš”ì•½": summary,
                    "ì–¸ë¡ ì‚¬": press,
                    "ë‚´ìš©ì ê²€": match_keyword_flag(full_text),
                    "ë³¸ë¬¸ë‚´_êµê³¼ì„œ_ë˜ëŠ”_ë°œí–‰ì‚¬_ì–¸ê¸‰": contains_textbook(full_text)
                })
            except:
                continue
        time.sleep(0.2)
    return pd.DataFrame(results)

def get_news_date(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        soup = BeautifulSoup(res.text, 'lxml')
        meta = soup.find("meta", {"property": "article:published_time"})
        if meta and meta.get("content"):
            return meta["content"][:10].replace("-", ".")
        return "ë‚ ì§œ ì—†ìŒ"
    except:
        return "ë‚ ì§œ ì˜¤ë¥˜"

def categorize_news(text):
    text = text.lower()
    for cat, words in category_keywords.items():
        if any(w in text for w in words):
            return cat
    return "ê¸°íƒ€"

def check_publisher(text):
    for pub in keywords:
        if pub.lower() in text:
            return pub
    return "ê¸°íƒ€"

def match_keyword_flag(text):
    for pub in keywords:
        if pub.lower() in text:
            return "O"
    return "X"

def contains_textbook(text):
    return "O" if "êµê³¼ì„œ" in text or "ë°œí–‰ì‚¬" in text else "X"

# ì¹´ì¹´ì˜¤í†¡ ë¶„ë¥˜ìš© í‚¤ì›Œë“œ
kakao_categories = {
    "ì±„íƒ: ì„ ì • ê¸°ì¤€/í‰ê°€": ["í‰ê°€í‘œ", "ê¸°ì¤€", "ì¶”ì²œì˜ê²¬ì„œ", "ì„ ì •ê¸°ì¤€"],
    "ì±„íƒ: ìœ„ì›íšŒ ìš´ì˜": ["ìœ„ì›íšŒ", "í˜‘ì˜íšŒ", "ëŒ€í‘œêµì‚¬", "ìœ„ì›"],
    "ì±„íƒ: íšŒì˜/ì‹¬ì˜ ì§„í–‰": ["íšŒì˜", "ì‹¬ì˜", "íšŒì˜ë¡", "ì‹¬ì‚¬"],
    "ë°°ì†¡": ["ë°°ì†¡", "ì™”ì–´ìš”", "ì „ì‹œë³¸", "ì§€ë„ì„œ"],
    "ì£¼ë¬¸": ["ê³µë¬¸", "ì •ì‚°", "ë‚˜ì´ìŠ¤", "ì—ë“€íŒŒì¸", "ë§ˆê°ì¼"],
    "ì¶œíŒì‚¬": ["ìë£Œ", "ê¸°í”„í‹°ì½˜", "êµì‚¬ìš©", "íšŒìˆ˜", "ìš”ì²­"]
}
publishers = ["ë¯¸ë˜ì—”", "ë¹„ìƒ", "ë™ì•„", "ì•„ì´ìŠ¤í¬ë¦¼", "ì²œì¬", "ì§€í•™ì‚¬"]
subjects = ["êµ­ì–´", "ìˆ˜í•™", "ì‚¬íšŒ", "ê³¼í•™", "ì˜ì–´"]
complaint_keywords = ["ì•ˆ ì™”ì–´ìš”", "ëŠ¦ê²Œ", "ì—†ì–´ìš”", "ë¬¸ì œ", "í—·ê°ˆë ¤", "ë¶ˆí¸"]

def analyze_kakao(text):
    date_line_pattern = re.compile(r"-+\s*(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼.*?-+")
    message_pattern = re.compile(
        r"\[(?P<sender>.*?)\]\s*\[(?P<ampm>ì˜¤ì „|ì˜¤í›„)\s*(?P<hour>\d{1,2}):(?P<minute>\d{2})\]\s*(?P<message>.*?)(?=
\[|\Z)",
        re.DOTALL
    )

    current_date = None
    results = []

    for line in text.splitlines():
        match = date_line_pattern.match(line)
        if match:
            year, month, day = map(int, match.groups())
            current_date = datetime(year, month, day).date()
            break
    if not current_date:
        current_date = datetime.today().date()  # âœ… ê¸°ë³¸ê°’ ì„¤ì •

    for match in message_pattern.finditer(text):
        sender = match.group("sender")
        ampm = match.group("ampm")
        hour = int(match.group("hour"))
        minute = match.group("minute")
        message = match.group("message").strip()

        if ampm == "ì˜¤í›„" and hour != 12:
            hour += 12
        elif ampm == "ì˜¤ì „" and hour == 12:
            hour = 0

        time_obj = datetime.strptime(f"{hour}:{minute}", "%H:%M").time()

        results.append({
            "ë‚ ì§œ": current_date,
            "ì‹œê°„": time_obj,
            "ë³´ë‚¸ ì‚¬ëŒ": sender,
            "ë©”ì‹œì§€": message,
            "ì¹´í…Œê³ ë¦¬": classify_category(message),
            "ì¶œíŒì‚¬": extract_kakao_publisher(message),
            "ê³¼ëª©": extract_subject(message),
            "ë¶ˆë§Œ ì—¬ë¶€": detect_complaint(message)
        })

    return pd.DataFrame(results)

def classify_category(text):
    for cat, words in kakao_categories.items():
        if any(w in text for w in words):
            return cat
    return "ê¸°íƒ€"

def extract_kakao_publisher(text):
    for pub in publishers:
        if pub in text:
            return pub
    return None

def extract_subject(text):
    for sub in subjects:
        if sub in text:
            return sub
    return None

def detect_complaint(text):
    return any(w in text for w in complaint_keywords)

# Streamlit ì•±
st.set_page_config(page_title="ğŸ“š êµê³¼ì„œ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ“š êµê³¼ì„œ ì»¤ë®¤ë‹ˆí‹° ë¶„ì„ & ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¬ì¸ì› ì•±")

tab1, tab2 = st.tabs(["ğŸ’¬ ì¹´ì¹´ì˜¤í†¡ ë¶„ì„", "ğŸ“° ë‰´ìŠ¤ í¬ë¡¤ë§"])

with tab1:
    st.subheader("ì¹´ì¹´ì˜¤í†¡ .txt íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="txt")
    if uploaded_file:
        raw_bytes = uploaded_file.read()
        detected = chardet.detect(raw_bytes)
        encoding = detected["encoding"] or "utf-8"
        text_raw = raw_bytes.decode(encoding, errors="ignore")

        st.write("ğŸ“Œ ê°ì§€ëœ ì¸ì½”ë”©:", encoding)
        df_kakao = analyze_kakao(text_raw)

        if df_kakao.empty:
            st.warning("âš ï¸ ë©”ì‹œì§€ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì´ë‚˜ ë‚´ìš© êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            st.success("âœ… ë¶„ì„ ì™„ë£Œ")
            st.write("ğŸ” ì´ ë¶„ì„ëœ ë©”ì‹œì§€ ìˆ˜:", len(df_kakao))
            st.dataframe(df_kakao)
            st.download_button("ğŸ“¥ ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", df_kakao.to_csv(index=False).encode("utf-8"), "ì¹´ì¹´ì˜¤í†¡_ë¶„ì„ê²°ê³¼.csv", "text/csv")

with tab2:
    st.subheader("ì¶œíŒì‚¬ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘ (ìµœê·¼ 2ì£¼)")
    if st.button("í¬ë¡¤ë§ ì‹œì‘"):
        progress = st.progress(0)
        collected = []
        with st.spinner("ğŸ•µï¸ ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”..."):
            for i, kw in enumerate(keywords):
                df = crawl_news_quick(kw, pages=5)
                collected.append(df)
                progress.progress((i + 1) / len(keywords))
        df_news = pd.concat(collected, ignore_index=True)
        st.success("âœ… ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ!")
        st.dataframe(df_news)
        st.download_button("ğŸ“¥ ë‰´ìŠ¤ ë°ì´í„° ë‹¤ìš´ë¡œë“œ", df_news.to_csv(index=False).encode("utf-8"), "ì¶œíŒì‚¬_ë‰´ìŠ¤_í¬ë¡¤ë§_ê²°ê³¼.csv", "text/csv")
