import streamlit as st
import pandas as pd
import requests
from datetime import datetime
from bs4 import BeautifulSoup
from io import StringIO
import re

# ----------------------------------
# ì¹´ì¹´ì˜¤í†¡ ë¶„ì„ í•¨ìˆ˜
# ----------------------------------
def analyze_kakao(text):
    pattern = re.compile(r"(?P<datetime>\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼ (ì˜¤ì „|ì˜¤í›„) \d{1,2}:\d{2}), (?P<sender>[^:]+) : (?P<message>.+)")
    matches = pattern.findall(text)

    rows = []
    for match in matches:
        date_str, ampm, sender, message = match
        try:
            dt = datetime.strptime(date_str.replace("ì˜¤ì „", "AM").replace("ì˜¤í›„", "PM"), "%Yë…„ %mì›” %dì¼ %p %I:%M")
            rows.append({
                "ë‚ ì§œ": dt.date(),
                "ì‹œê°„": dt.time(),
                "ë³´ë‚¸ ì‚¬ëŒ": sender.strip(),
                "ë©”ì‹œì§€": message.strip()
            })
        except:
            continue

    return pd.DataFrame(rows)

# ----------------------------------
# ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
# ----------------------------------
def crawl_news(keyword):
    url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sort=1&nso=so%3Add%2Cp%3A2w"
    headers = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, "lxml")
    articles = soup.select(".news_area")

    results = []
    for article in articles[:10]:  # ìµœëŒ€ 10ê°œ ê¸°ì‚¬
        try:
            title = article.select_one(".news_tit").get("title")
            link = article.select_one(".news_tit").get("href")
            summary = article.select_one(".dsc_txt_wrap").text
            press = article.select_one(".info_group a").text
            results.append({
                "ì œëª©": title,
                "ìš”ì•½": summary,
                "URL": link,
                "ì–¸ë¡ ì‚¬": press
            })
        except:
            continue
    return pd.DataFrame(results)

# ----------------------------------
# Streamlit UI
# ----------------------------------
st.set_page_config(page_title="ì˜¬ì¸ì› êµê³¼ì„œ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ“š êµê³¼ì„œ ì»¤ë®¤ë‹ˆí‹° ë¶„ì„ & ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¬ì¸ì› ì•±")

tab1, tab2 = st.tabs(["ğŸ’¬ ì¹´ì¹´ì˜¤í†¡ ë¶„ì„", "ğŸ“° ë‰´ìŠ¤ í¬ë¡¤ë§"])

# ---------------------------
# ì¹´ì¹´ì˜¤í†¡ íƒ­
# ---------------------------
with tab1:
    st.subheader("ğŸ“‚ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ë¶„ì„ê¸°")
    uploaded_file = st.file_uploader("ì¹´ì¹´ì˜¤í†¡ .txt íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="txt")
    if uploaded_file:
        stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
        df_kakao = analyze_kakao(stringio.read())
        st.success("âœ… ëŒ€í™” ë¶„ì„ ì™„ë£Œ")
        st.dataframe(df_kakao)

        st.download_button(
            "ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
            data=df_kakao.to_csv(index=False).encode("utf-8"),
            file_name="ì¹´ì¹´ì˜¤í†¡_ë¶„ì„ê²°ê³¼.csv",
            mime="text/csv"
        )

# ---------------------------
# ë‰´ìŠ¤ í¬ë¡¤ë§ íƒ­
# ---------------------------
with tab2:
    st.subheader("ğŸ“° ì¶œíŒì‚¬ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° (ìµœê·¼ 2ì£¼)")
    keyword_input = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„)", "ì²œì¬êµìœ¡, ì²œì¬êµê³¼ì„œ, ë¯¸ë˜ì—”, êµê³¼ì„œ")
    if st.button("í¬ë¡¤ë§ ì‹œì‘"):
        keywords = [kw.strip() for kw in keyword_input.split(",") if kw.strip()]
        all_news = []

        for kw in keywords:
            news_df = crawl_news(kw)
            news_df["ê²€ìƒ‰ì–´"] = kw
            all_news.append(news_df)

        if all_news:
            df_news = pd.concat(all_news, ignore_index=True)
            st.success("âœ… ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ")
            st.dataframe(df_news)

            st.download_button(
                "ğŸ“¥ ë‰´ìŠ¤ ë°ì´í„° ë‹¤ìš´ë¡œë“œ",
                data=df_news.to_csv(index=False).encode("utf-8"),
                file_name="ì¶œíŒì‚¬_ë‰´ìŠ¤_ê²°ê³¼.csv",
                mime="text/csv"
            )
        else:
            st.warning("í¬ë¡¤ë§ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
