from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
from datetime import datetime
import requests
import pandas as pd
import time

CHROMEDRIVER_PATH = "C:/Users/mungk/chromedriver-win64/chromedriver.exe"
keywords = ["ì²œì¬êµìœ¡", "ì²œì¬êµê³¼ì„œ", "ì§€í•™ì‚¬", "ë²½í˜¸", "í”„ë¦°í”¼ì•„", "ë¯¸ë˜ì—”", "êµê³¼ì„œ", "ë™ì•„ì¶œíŒ"]

options = Options()
options.add_argument("--headless")
options.add_argument("--disable-gpu")
service = Service(CHROMEDRIVER_PATH)
driver = webdriver.Chrome(service=service, options=options)

results = []
seen_urls = set()
seen_summaries = set()

# âœ… ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í•¨ìˆ˜
def categorize(text):
    text = text.lower()
    if "ì´ë²¤íŠ¸" in text:
        return "ì´ë²¤íŠ¸"
    elif "í›„ì›" in text or "ê¸°íƒ" in text:
        return "í›„ì›"
    elif "ê¸°ë¶€" in text:
        return "ê¸°ë¶€"
    elif "í˜‘ì•½" in text or "mou" in text:
        return "í˜‘ì•½/MOU"
    elif any(word in text for word in ["ì—ë“€í…Œí¬", "ë””ì§€í„¸êµìœ¡", "ë””ì§€í„¸ êµìœ¡", "aiêµìœ¡", "ai êµìœ¡", "ìŠ¤ë§ˆíŠ¸êµìœ¡", "ìŠ¤ë§ˆíŠ¸ êµìœ¡"]):
        return "ì—ë“€í…Œí¬/ë””ì§€í„¸êµìœ¡"
    elif "ì •ì±…" in text:
        return "ì •ì±…"
    elif "ì¶œíŒ" in text:
        return "ì¶œíŒ"
    elif "ì¸ì‡„" in text or "í”„ë¦°íŠ¸" in text:
        return "í”„ë¦°íŠ¸ ë° ì¸ì‡„"
    elif "ì±„ìš©" in text or "êµì‚¬" in text:
        return "ì¸ì‚¬/ì±„ìš©"
    elif "ê³µê¸‰" in text:
        return "ê³µê¸‰"
    elif "êµìœ¡" in text:
        return "êµìœ¡"
    else:
        return "ê¸°íƒ€"

# âœ… ì¶œíŒì‚¬ëª… ì¶”ì¶œ í•¨ìˆ˜
def extract_publisher(text):
    text = text.lower()
    for pub in keywords:
        if pub.lower() in text:
            return pub
    return "ê¸°íƒ€"

# âœ… ë‚ ì§œ ì¶”ì¶œ í•¨ìˆ˜ (ë©”íƒ€íƒœê·¸ ê¸°ë°˜)
def get_date_from_news_url(news_url):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(news_url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'lxml')

        meta_tags = [
            ("meta", {"property": "article:published_time"}),
            ("meta", {"name": "date"}),
            ("meta", {"property": "og:article:published_time"})
        ]
        for tag, attrs in meta_tags:
            meta = soup.find(tag, attrs)
            if meta and meta.get("content"):
                return meta["content"][:10].replace("-", ".")

        text = soup.get_text(" ", strip=True)
        for word in text.split():
            for fmt in ["%Y.%m.%d", "%Y-%m-%d", "%Y/%m/%d"]:
                try:
                    dt = datetime.strptime(word[:10], fmt)
                    return dt.strftime("%Y.%m.%d")
                except:
                    continue
        return "ë‚ ì§œ ì—†ìŒ"
    except:
        return "ë‚ ì§œ ì˜¤ë¥˜"

# âœ… ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
def get_body_text(news_url):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(news_url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'lxml')
        candidates = ["article", "article-body", "newsEndContents", "content", "viewContent"]
        for class_name in candidates:
            tag = soup.find(class_=class_name)
            if tag:
                return tag.get_text(separator=" ", strip=True).lower()
        return soup.get_text(separator=" ", strip=True).lower()
    except:
        return ""

# âœ… ì¶œíŒì‚¬ ê´€ë ¨ ì—¬ë¶€ í™•ì¸
def is_related_to_publisher(text):
    text = text.lower()
    for pub in keywords:
        if pub.lower() in text:
            return "O"
    return "X"

# âœ… ë³¸ë¬¸ ë‚´ êµê³¼ì„œ ë˜ëŠ” ë°œí–‰ì‚¬ ì–¸ê¸‰ ì—¬ë¶€
def check_textbook_or_publisher_in_body(body_text):
    if "êµê³¼ì„œ" in body_text or "ë°œí–‰ì‚¬" in body_text:
        return "O"
    return "X"

# âœ… ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰
for keyword in keywords:
    print(f"ğŸ” ê²€ìƒ‰ì–´: {keyword}")

    for page in range(1, 21):
        start = (page - 1) * 10 + 1
        url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sort=1&nso=so%3Add%2Cp%3A2w&start={start}"
        driver.get(url)
        time.sleep(1)

        soup = BeautifulSoup(driver.page_source, 'lxml')
        articles = soup.select(".news_area")

        print(f"ğŸ“„ {keyword} {page}í˜ì´ì§€ ê¸°ì‚¬ ìˆ˜: {len(articles)}")
        if not articles:
            break

        for article in articles:
            try:
                title_elem = article.select_one(".news_tit")
                title = title_elem.get("title")
                link = title_elem.get("href")
                if link in seen_urls:
                    continue
                seen_urls.add(link)

                summary = article.select_one(".dsc_txt_wrap").get_text(strip=True) if article.select_one(".dsc_txt_wrap") else ""
                if summary in seen_summaries:
                    continue
                seen_summaries.add(summary)

                press = article.select_one(".info_group a").get_text(strip=True) if article.select_one(".info_group a") else ""
                body_text = get_body_text(link)
                full_text = (summary + " " + body_text).strip()

                date = get_date_from_news_url(link)
                publisher = extract_publisher(full_text)
                category = categorize(full_text)
                match_check = is_related_to_publisher(full_text)
                body_check = check_textbook_or_publisher_in_body(body_text)

                results.append({
                    "ì¶œíŒì‚¬ëª…": publisher,
                    "ì¹´í…Œê³ ë¦¬": category,
                    "ë‚ ì§œ": date,
                    "ì œëª©": title,
                    "URL": link,
                    "ìš”ì•½": summary,
                    "ì–¸ë¡ ì‚¬": press,
                    "ë‚´ìš©ì ê²€": match_check,
                    "ë³¸ë¬¸ë‚´_êµê³¼ì„œ_ë˜ëŠ”_ë°œí–‰ì‚¬_ì–¸ê¸‰": body_check
                })
            except Exception as e:
                print(f"[ì˜¤ë¥˜] {keyword} {page}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")
        time.sleep(0.3)

driver.quit()
df = pd.DataFrame(results)
df.to_excel("ì¶œíŒì‚¬_ë‰´ìŠ¤_í¬ë¡¤ë§_ê²°ê³¼.xlsx", index=False, engine="openpyxl")
print("âœ… í¬ë¡¤ë§ ì™„ë£Œ! 'ì¶œíŒì‚¬_ë‰´ìŠ¤_í¬ë¡¤ë§_ê²°ê³¼.xlsx' ìƒì„±ë¨.")
