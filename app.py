import streamlit as st
import pandas as pd
import requests
from datetime import datetime
from bs4 import BeautifulSoup
from io import StringIO
import re
import time

# â–¶ í¬ë¡¤ë§ í‚¤ì›Œë“œ (êµ­ì •êµê³¼ì„œ â†’ êµê³¼ì„œë¡œ í†µì¼)
keywords = ["ì²œì¬êµìœ¡", "ì²œì¬êµê³¼ì„œ", "ì§€í•™ì‚¬", "ë²½í˜¸", "í”„ë¦°í”¼ì•„", "ë¯¸ë˜ì—”", "êµê³¼ì„œ", "ë™ì•„ì¶œíŒ"]

# â–¶ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
def categorize(text):
    text = text.lower()
    if "ì´ë²¤íŠ¸" in text:
        return "ì´ë²¤íŠ¸"
    elif "í›„ì›" in text or "ê¸°íƒ" in text:
        return "í›„ì›"
    elif "ê¸°ë¶€" in text:
        return "ê¸°ë¶€"
    elif "í˜‘ì•½" in text or "mou" in text:
        return "í˜‘ì•½/MOU"
    elif any(w in text for w in ["ì—ë“€í…Œí¬", "ë””ì§€í„¸êµìœ¡", "aiêµìœ¡", "ìŠ¤ë§ˆíŠ¸êµìœ¡"]):
        return "ì—ë“€í…Œí¬/ë””ì§€í„¸êµìœ¡"
    elif "ì •ì±…" in text:
        return "ì •ì±…"
    elif "ì¶œíŒ" in text:
        return "ì¶œíŒ"
    elif "ì¸ì‡„" in text or "í”„ë¦°íŠ¸" in text:
        return "í”„ë¦°íŠ¸ ë° ì¸ì‡„"
    elif "ì±„ìš©" in text or "êµì‚¬" in text:
        return "ì¸ì‚¬/ì±„ìš©"
    elif "ê³µê¸‰" in text:
        return "ê³µê¸‰"
    elif "êµìœ¡" in text:
        return "êµìœ¡"
    else:
        return "ê¸°íƒ€"

def extract_publisher(text):
    text = text.lower()
    for pub in keywords:
        if pub.lower() in text:
            return pub
    return "ê¸°íƒ€"

def check_textbook_or_publisher_in_body(text):
    if "êµê³¼ì„œ" in text or "ë°œí–‰ì‚¬" in text:
        return "O"
    return "X"

def get_news_date(news_url):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(news_url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, "lxml")
        for attr in [{"property": "article:published_time"}, {"name": "date"}, {"property": "og:article:published_time"}]:
            tag = soup.find("meta", attr)
            if tag and tag.get("content"):
                return tag["content"][:10].replace("-", ".")
        # fallback
        text = soup.get_text(" ", strip=True)
        for word in text.split():
            for fmt in ["%Y.%m.%d", "%Y-%m-%d", "%Y/%m/%d"]:
                try:
                    return datetime.strptime(word[:10], fmt).strftime("%Y.%m.%d")
                except:
                    continue
        return "ë‚ ì§œ ì—†ìŒ"
    except:
        return "ë‚ ì§œ ì˜¤ë¥˜"

def get_body_text(news_url):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(news_url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, "lxml")
        candidates = ["article", "article-body", "newsEndContents", "content", "viewContent"]
        for c in candidates:
            tag = soup.find(class_=c)
            if tag:
                return tag.get_text(separator=" ", strip=True).lower()
        return soup.get_text(separator=" ", strip=True).lower()
    except:
        return ""

def crawl_news(keywords):
    seen_urls = set()
    seen_summaries = set()
    results = []

    for keyword in keywords:
        for page in range(1, 21):
            start = (page - 1) * 10 + 1
            url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sort=1&nso=so%3Add%2Cp%3A2y&start={start}"
            headers = {"User-Agent": "Mozilla/5.0"}
            try:
                res = requests.get(url, headers=headers, timeout=5)
                soup = BeautifulSoup(res.text, "lxml")
                articles = soup.select(".news_area")
                if not articles:
                    break
                for article in articles:
                    try:
                        title_elem = article.select_one(".news_tit")
                        title = title_elem.get("title")
                        link = title_elem.get("href")
                        summary = article.select_one(".dsc_txt_wrap").text
                        press = article.select_one(".info_group a").text

                        if link in seen_urls or summary in seen_summaries:
                            continue
                        seen_urls.add(link)
                        seen_summaries.add(summary)

                        body = get_body_text(link)
                        full_text = summary + " " + body
                        date = get_news_date(link)

                        results.append({
                            "ì¶œíŒì‚¬ëª…": extract_publisher(full_text),
                            "ì¹´í…Œê³ ë¦¬": categorize(full_text),
                            "ë‚ ì§œ": date,
                            "ì œëª©": title,
                            "URL": link,
                            "ìš”ì•½": summary,
                            "ì–¸ë¡ ì‚¬": press,
                            "ë‚´ìš©ì ê²€": "O" if extract_publisher(full_text) != "ê¸°íƒ€" else "X",
                            "ë³¸ë¬¸ë‚´_êµê³¼ì„œ_ë˜ëŠ”_ë°œí–‰ì‚¬_ì–¸ê¸‰": check_textbook_or_publisher_in_body(body)
                        })
                        time.sleep(0.3)
                    except:
                        continue
            except:
                continue
    return pd.DataFrame(results)

# ===============================
# ğŸ’¬ ì¹´ì¹´ì˜¤í†¡ ë¶„ì„ í•¨ìˆ˜
# ===============================
def analyze_kakao(text):
    pattern = re.compile(r"(?P<datetime>\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼ (ì˜¤ì „|ì˜¤í›„) \d{1,2}:\d{2}), (?P<sender>[^:]+) : (?P<message>.+)")
    matches = pattern.findall(text)
    rows = []
    for match in matches:
        date_str, ampm, sender, message = match
        try:
            dt = datetime.strptime(date_str.replace("ì˜¤ì „", "AM").replace("ì˜¤í›„", "PM"), "%Yë…„ %mì›” %dì¼ %p %I:%M")
            rows.append({
                "ë‚ ì§œ": dt.date(),
                "ì‹œê°„": dt.time(),
                "ë³´ë‚¸ ì‚¬ëŒ": sender.strip(),
                "ë©”ì‹œì§€": message.strip()
            })
        except:
            continue
    return pd.DataFrame(rows)

# ===============================
# ğŸ–¥ï¸ Streamlit ì•± ì‹¤í–‰ í™”ë©´
# ===============================
st.set_page_config(page_title="ì˜¬ì¸ì› êµê³¼ì„œ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ“š êµê³¼ì„œ ì»¤ë®¤ë‹ˆí‹° ë¶„ì„ & ë‰´ìŠ¤ ìˆ˜ì§‘")

tab1, tab2 = st.tabs(["ğŸ’¬ ì¹´ì¹´ì˜¤í†¡ ë¶„ì„", "ğŸ“° ë‰´ìŠ¤ í¬ë¡¤ë§"])

# â–¶ ì¹´ì¹´ì˜¤í†¡ íƒ­
with tab1:
    st.subheader("ğŸ’¬ ì¹´ì¹´ì˜¤í†¡ ë‹¨í†¡ë°© .txt íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("íŒŒì¼ ì„ íƒ", type="txt")
    if uploaded_file:
        stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
        df_kakao = analyze_kakao(stringio.read())
        st.success("âœ… ëŒ€í™” ë¶„ì„ ì™„ë£Œ!")
        st.dataframe(df_kakao)

        st.download_button(
            label="ğŸ“¥ ë¶„ì„ê²°ê³¼ ë‹¤ìš´ë¡œë“œ",
            data=df_kakao.to_csv(index=False).encode("utf-8"),
            file_name="ì¹´ì¹´ì˜¤í†¡_ë¶„ì„ê²°ê³¼.csv",
            mime="text/csv"
        )

# â–¶ ë‰´ìŠ¤ í¬ë¡¤ë§ íƒ­
with tab2:
    st.subheader("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ì¶œíŒì‚¬ ê´€ë ¨ ê¸°ì‚¬ ìˆ˜ì§‘ (ìµœê·¼ 2ë…„)")
    extra_kw = st.text_input("ì¶”ê°€ ê²€ìƒ‰ì–´ ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„)", "")
    run = st.button("ğŸ” ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")

    if run:
        user_keywords = [kw.strip() for kw in extra_kw.split(",") if kw.strip()]
        all_keywords = list(set(keywords + user_keywords))

        df_news = crawl_news(all_keywords)
        if not df_news.empty:
            st.success("âœ… ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ")
            st.dataframe(df_news)

            st.download_button(
                label="ğŸ“¥ ë‰´ìŠ¤ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ",
                data=df_news.to_csv(index=False).encode("utf-8"),
                file_name="ì¶œíŒì‚¬_ë‰´ìŠ¤_í¬ë¡¤ë§_ê²°ê³¼.csv",
                mime="text/csv"
            )
        else:
            st.warning("âŒ ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
