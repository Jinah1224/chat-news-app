import streamlit as st
import pandas as pd
import re
from datetime import datetime, timedelta
import chardet
import requests
from bs4 import BeautifulSoup
import time

st.set_page_config(page_title="ğŸ“š AI ê¸°ë°˜ êµê³¼ì„œ ê´€ë ¨ ë™í–¥ ë¶„ì„ê¸°", layout="wide")
st.markdown("""
    <style>
    .stMultiSelect > div > div {
        border-radius: 1rem;
        background-color: #f0f2f6;
        padding: 0.4rem 0.6rem;
    }
    .stMultiSelect div[data-baseweb="tag"] {
        background-color: #eef0f4;
        color: #333;
        border-radius: 8px;
        font-weight: 500;
    }
    .stMultiSelect div[data-baseweb="tag"] span {
        font-size: 14px;
    }
    </style>
""", unsafe_allow_html=True)

st.title("ğŸ“š ì¹´ì¹´ì˜¤í†¡ ë¶„ì„ + ë‰´ìŠ¤ ìˆ˜ì§‘ í†µí•© ì•±")

# -------------------------------
# ì¹´ì¹´ì˜¤í†¡ ë¶„ì„ ê¸°ì¤€ ë° í•¨ìˆ˜
# -------------------------------
kakao_categories = {
    "ì±„íƒ: ì„ ì • ê¸°ì¤€/í‰ê°€": ["í‰ê°€í‘œ", "ê¸°ì¤€", "ì¶”ì²œì˜ê²¬ì„œ", "ì„ ì •ê¸°ì¤€"],
    "ì±„íƒ: ìœ„ì›íšŒ ìš´ì˜": ["ìœ„ì›íšŒ", "í˜‘ì˜íšŒ", "ëŒ€í‘œêµì‚¬", "ìœ„ì›"],
    "ì±„íƒ: íšŒì˜/ì‹¬ì˜ ì§„í–‰": ["íšŒì˜", "íšŒì˜ë¡", "ì‹¬ì˜", "ì‹¬ì‚¬", "ìš´ì˜"],
    "ë°°ì†¡": ["ë°°ì†¡"],
    "ë°°ì†¡: ì§€ë„ì„œ/ì „ì‹œë³¸ ë„ì°©": ["ë„ì°©", "ì™”ì–´ìš”", "ì „ì‹œë³¸", "ì§€ë„ì„œ", "ë°•ìŠ¤"],
    "ë°°ì†¡: ë¼ë²¨/ì •ë¦¬ ì—…ë¬´": ["ë¼ë²¨", "ë¶„ë¥˜", "ì •ë¦¬", "ì „ì‹œ ì¤€ë¹„"],
    "ì£¼ë¬¸: ì‹œìŠ¤í…œ ì‚¬ìš©": ["ë‚˜ì´ìŠ¤", "ì—ë“€íŒŒì¸", "ë“±ë¡", "ì…ë ¥"],
    "ì£¼ë¬¸: ê³µë¬¸/ì •ì‚°": ["ê³µë¬¸", "ì •ì‚°", "ë§ˆê°ì¼", "ìš”ì²­"],
    "ì¶œíŒì‚¬: ìë£Œ ìˆ˜ë ¹/ì´ë²¤íŠ¸": ["ë³´ì¡°ìë£Œ", "ìë£Œ", "ê¸°í”„í‹°ì½˜", "ì´ë²¤íŠ¸"],
    "ì¶œíŒì‚¬: ìë£Œ íšŒìˆ˜/ìš”ì²­": ["íšŒìˆ˜", "ìš”ì²­", "êµì‚¬ìš©"]
}
publishers = ["ë¯¸ë˜ì—”", "ë¹„ìƒ", "ë™ì•„", "ì•„ì´ìŠ¤í¬ë¦¼", "ì²œì¬", "ì¢‹ì€ì±…", "ì§€í•™ì‚¬", "ëŒ€êµ", "ì´ë£¸", "ëª…ì§„", "ì²œì¬êµìœ¡"]
subjects = ["êµ­ì–´", "ìˆ˜í•™", "ì‚¬íšŒ", "ê³¼í•™", "ì˜ì–´", "ë„ë•", "ìŒì•…", "ë¯¸ìˆ ", "ì²´ìœ¡"]
complaint_keywords = ["ì•ˆ ì™”ì–´ìš”", "ì•„ì§", "ëŠ¦ê²Œ", "ì—†ì–´ìš”", "ì˜¤ë¥˜", "ë¬¸ì œ", "ì™œ", "í—·ê°ˆë ¤", "ë¶ˆí¸", "ì•ˆì˜´", "ì§€ì—°", "ì•ˆë³´ì—¬ìš”", "ëª» ë°›ì•˜", "í˜ë“¤ì–´ìš”"]

def classify_category(text):
    for cat, words in kakao_categories.items():
        if any(w in text for w in words):
            return cat
    return "ê¸°íƒ€"

def extract_kakao_publisher(text):
    for pub in publishers:
        if pub in text:
            return pub
    return None

def extract_subject(text):
    for sub in subjects:
        if sub in text:
            return sub
    return None

def detect_complaint(text):
    return any(w in text for w in complaint_keywords)


def parse_kakao_text(text):
    parsed = []
    pattern1 = re.compile(r"(\d{4})ë…„ (\d{1,2})ì›” (\d{1,2})ì¼ (ì˜¤ì „|ì˜¤í›„)? (\d{1,2}):(\d{2}), (.+?) : (.+)")
    pattern2 = re.compile(r"\[(.*?)\] \[(ì˜¤ì „|ì˜¤í›„) (\d{1,2}):(\d{2})\] (.+)")
    date_pattern = re.compile(r"-+ (\d{4})ë…„ (\d{1,2})ì›” (\d{1,2})ì¼")
    lines = text.splitlines()
    current_date = None
    for line in lines:
        if m1 := pattern1.match(line):
            y, m, d, ampm, h, mi, sender, msg = m1.groups()
            h = int(h)
            mi = int(mi)
            if ampm == "ì˜¤í›„" and h != 12:
                h += 12
            elif ampm == "ì˜¤ì „" and h == 12:
                h = 0
            dt = datetime(int(y), int(m), int(d), h, mi)
            if sender.strip() != "ì˜¤í”ˆì±„íŒ…ë´‡":
                parsed.append({
                    "ë‚ ì§œ": dt.date(), "ì‹œê°„": dt.time(),
                    "ë³´ë‚¸ ì‚¬ëŒ": sender.strip(), "ë©”ì‹œì§€": msg.strip(),
                    "ì¹´í…Œê³ ë¦¬": classify_category(msg),
                    "ì¶œíŒì‚¬": extract_kakao_publisher(msg),
                    "ê³¼ëª©": extract_subject(msg),
                    "ë¶ˆë§Œ ì—¬ë¶€": detect_complaint(msg)
                })
        elif m2 := pattern2.match(line):
            sender, ampm, h, mi, msg = m2.groups()
            if current_date and sender.strip() != "ì˜¤í”ˆì±„íŒ…ë´‡":
                h = int(h)
                mi = int(mi)
                if ampm == "ì˜¤í›„" and h != 12:
                    h += 12
                elif ampm == "ì˜¤ì „" and h == 12:
                    h = 0
                t = datetime.strptime(f"{h}:{mi}", "%H:%M").time()
                parsed.append({
                    "ë‚ ì§œ": current_date, "ì‹œê°„": t,
                    "ë³´ë‚¸ ì‚¬ëŒ": sender.strip(), "ë©”ì‹œì§€": msg.strip(),
                    "ì¹´í…Œê³ ë¦¬": classify_category(msg),
                    "ì¶œíŒì‚¬": extract_kakao_publisher(msg),
                    "ê³¼ëª©": extract_subject(msg),
                    "ë¶ˆë§Œ ì—¬ë¶€": detect_complaint(msg)
                })
        elif d := date_pattern.match(line):
            y, m, d = map(int, d.groups())
            current_date = datetime(y, m, d).date()
    return pd.DataFrame(parsed)

# -------------------------------
# ë‰´ìŠ¤ ê´€ë ¨ í¬ë¡¤ë§ í•¨ìˆ˜
# -------------------------------
def get_news_date(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        soup = BeautifulSoup(res.text, 'lxml')
        meta = soup.find("meta", {"property": "article:published_time"})
        if meta and meta.get("content"):
            return meta["content"][:10].replace("-", ".")
        return "ë‚ ì§œ ì—†ìŒ"
    except:
        return "ë‚ ì§œ ì˜¤ë¥˜"

keywords = ["ì²œì¬êµìœ¡", "ì²œì¬êµê³¼ì„œ", "ì§€í•™ì‚¬", "ë²½í˜¸", "í”„ë¦°í”¼ì•„", "ë¯¸ë˜ì—”", "êµê³¼ì„œ", "ë™ì•„ì¶œíŒ"]
category_keywords = {
    "í›„ì›": ["í›„ì›", "ê¸°íƒ"], "ê¸°ë¶€": ["ê¸°ë¶€"], "í˜‘ì•½/MOU": ["í˜‘ì•½", "mou"],
    "ì—ë“€í…Œí¬/ë””ì§€í„¸êµìœ¡": ["ì—ë“€í…Œí¬", "ë””ì§€í„¸êµìœ¡", "aiêµìœ¡", "ìŠ¤ë§ˆíŠ¸êµìœ¡"],
    "ì •ì±…": ["ì •ì±…"], "ì¶œíŒ": ["ì¶œíŒ"], "ì¸ì‚¬/ì±„ìš©": ["ì±„ìš©", "êµì‚¬"],
    "í”„ë¦°íŠ¸ ë° ì¸ì‡„": ["ì¸ì‡„", "í”„ë¦°íŠ¸"], "ê³µê¸‰": ["ê³µê¸‰"], "êµìœ¡": ["êµìœ¡"], "ì´ë²¤íŠ¸": ["ì´ë²¤íŠ¸", "ì‚¬ì€í’ˆ"]
}

def crawl_news_quick(keyword, pages=3):
    headers = {"User-Agent": "Mozilla/5.0"}
    results = []
    seen = set()
    today = datetime.today().date()
    two_weeks_ago = today - timedelta(days=14)
    for page in range(1, pages + 1):
        start = (page - 1) * 10 + 1
        url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sort=1&nso=so:dd,p:2w&start={start}"
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, "lxml")
        articles = soup.select(".news_area")
        for a in articles:
            try:
                title = a.select_one(".news_tit").get("title")
                link = a.select_one(".news_tit").get("href")
                summary = a.select_one(".dsc_txt_wrap").get_text(strip=True)
                press = a.select_one(".info_group a").get_text(strip=True)
                if link in seen or summary in seen:
                    continue
                seen.add(link)
                seen.add(summary)
                date = get_news_date(link)
                full_text = (title + " " + summary).lower()
                results.append({
                    "ì¶œíŒì‚¬ëª…": check_publisher(full_text),
                    "ì¹´í…Œê³ ë¦¬": categorize_news(full_text),
                    "ë‚ ì§œ": date,
                    "ì œëª©": title,
                    "URL": link,
                    "ìš”ì•½": summary,
                    "ì–¸ë¡ ì‚¬": press,
                    "ë³¸ë¬¸ë‚´_êµê³¼ì„œ_ë˜ëŠ”_ë°œí–‰ì‚¬_ì–¸ê¸‰": contains_textbook(full_text)
                })
            except:
                continue
        time.sleep(0.3)
    return pd.DataFrame(results)

def categorize_news(text):
    text = text.lower()
    for cat, words in category_keywords.items():
        if any(w in text for w in words):
            return cat
    return "ê¸°íƒ€"

def check_publisher(text):
    for pub in keywords:
        if pub.lower() in text:
            return pub
    return "ê¸°íƒ€"

def contains_textbook(text):
    return "O" if "êµê³¼ì„œ" in text or "ë°œí–‰ì‚¬" in text else "X"

# -------------------------------
# Streamlit UI
# -------------------------------
tab1, tab2 = st.tabs(["ğŸ’¬ ì¹´ì¹´ì˜¤í†¡ ë¶„ì„", "ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘"])

with tab1:
    st.subheader("ì¹´ì¹´ì˜¤í†¡ .txt ì—…ë¡œë“œ")
    uploaded = st.file_uploader("ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ", type="txt")
    if uploaded:
        raw_bytes = uploaded.read()
        encoding = chardet.detect(raw_bytes)["encoding"] or "utf-8"
        text = raw_bytes.decode(encoding, errors="ignore")
        df_kakao = parse_kakao_text(text)
        if df_kakao.empty:
            st.warning("â— ë©”ì‹œì§€ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.success(f"âœ… ì´ {len(df_kakao)}ê°œ ë©”ì‹œì§€ ë¶„ì„ ì™„ë£Œ!")
            st.dataframe(df_kakao)
            st.download_button("ğŸ“¥ ì—‘ì…€ ì €ì¥", df_kakao.to_excel(index=False), "kakao_cleaned.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

with tab2:
    st.subheader("ì¶œíŒì‚¬ ê´€ë ¨ ë‰´ìŠ¤ í¬ë¡¤ë§(ìµœê·¼ 2ì£¼)")
    st.markdown("ğŸ“ **ê¸°ë³¸ ìˆ˜ì§‘ í‚¤ì›Œë“œì—ì„œ ì„ íƒí•  ìˆ˜ ìˆì–´ìš”.**")
    selected_keywords = st.multiselect("ğŸ” ê¸°ë³¸ í‚¤ì›Œë“œ ì„ íƒ", keywords, default=keywords)
    all_selected_keywords = selected_keywords.copy()
    if not all_selected_keywords:
        st.warning("â— í•˜ë‚˜ ì´ìƒì˜ í‚¤ì›Œë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
    else:
        if st.button("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘"):
            progress = st.progress(0)
            all_news = []
            for i, kw in enumerate(all_selected_keywords):
                df = crawl_news_quick(kw)
                all_news.append(df)
                progress.progress((i+1)/len(all_selected_keywords))
            df_news = pd.concat(all_news, ignore_index=True)
            st.success("âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ!")
            st.dataframe(df_news)
            st.download_button("ğŸ“¥ ë‰´ìŠ¤ ì—‘ì…€ ì €ì¥", df_news.to_excel(index=False), "news_result.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
